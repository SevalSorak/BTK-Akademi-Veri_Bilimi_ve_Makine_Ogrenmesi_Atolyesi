{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801e7614",
   "metadata": {},
   "source": [
    "# Pekiştirmeli Öğrenme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a5166",
   "metadata": {},
   "source": [
    "* Pekiştirmeli öğrenme, davranışçılıktan esinlenen, öznelerin bir ortamda en yüksek ödül miktarına ulaşabilmesi için hangi eylemleri yapması gerektiğiyle ilgilenen bir makine öğrenmesi yaklaşımıdır.\n",
    "* Ajan olarak adlandırılan model, çevresiyle etkileşime girerek, daha önceden belirlenen ödül - ceza parametreleri sayesinde, problemin çözümü için gerekli olan eylemleri nasıl yapacağını öğrenir.\n",
    "* Bu bölümde Pekiştirmeli Öğrenme'nin temellerinden olan Q -Öğrenme yöntemini kullanacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c326b0",
   "metadata": {},
   "source": [
    "## Durum, Eylem, Ödül, Deterministik ve Stokastik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53faf1",
   "metadata": {},
   "source": [
    "* Q-öğrenmede amaç, maksimum değeri bularak ayrı bir eylemler dizisinden tek bir deterministik eylem öğrenmektir.\n",
    "* Diğer bir deyişle ödülü en üst düzeye çıkaran en uygun yolu bulmak\n",
    "* Kullanılan terimler:\n",
    "    * Ajan: Çevrede eylem gerçekleştiren model.\n",
    "    * Çevre: Ajanın eylemlerini gerçekleştirebildiği ortam, mesela 2 boyutlu bir oda.\n",
    "    * Durum: Ajanın çevrede içerisinde bulunduğu durum, mesela 2 boyutlu bir odada x, y konumu.\n",
    "    * Eylem: Hareket, mesela 2 boyutlu bir odada ileri ya da geri gitmek.\n",
    "    * Ödül: Ajanın eylemleri sonucunda çevreden aldığı geri bildirim, mesela odada ulaşması gereken konuma ulaşınca aldığı +5 puan ya da odanın içerisinde çukura düşünce aldığı -5 puan.\n",
    "    * Deterministik ve Stokastik: Ajanın hareketlerinin rastgelelik bulunmaması ya da rastgele olması"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc593cfe",
   "metadata": {},
   "source": [
    "## Bellman Denklemi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b6775",
   "metadata": {},
   "source": [
    "* Belirli bir durumun değeri, belirli bir durumda optimum eylemin ödülünün maksimum eylemine eşittir ve bir indirim faktörü, Bellman Denkelmi'nden sonraki durumun değeri ile çarpılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b90b8",
   "metadata": {},
   "source": [
    "## Markov Karar Süreci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf22889",
   "metadata": {},
   "source": [
    "* Markov özelligi, gelecegin gecmise degil, yalnizca bugüne bagli oldugunu belirtir.\n",
    "* Markov zinciri, önceki durumlara degil, yalnizca mevcut duruma bagli olan, yani gelecek koşullu olarak geçmisten bagimsiz olan olasiliksal bir modeldir.\n",
    "* MKS, Markov zincirinin bir uzantisidir. Karar verme durumlarini modellemek icin matematiksel bir çerceve saglar. Hemen hemen tüm Pekistirmeli Ögrenme problemleri MMKSDP olarak modellenebilir. MKS, 5 Onemli unsurla temsil edilebilir:\n",
    "    1. Temsilcinin içinde olabilecegi bir dizi durum \n",
    "    2. Bir durumdan digerine geçmek icin bir aracı tarafindan gerçeklestirilebilecek bir dizi eylem \n",
    "    3. Bir eylem gerceklestirerek bir durumdan digerine geçme olasiligi olan bir geciş olasiligi \n",
    "    4. Bir eylemi gerçeklestirerek bir durumdan baska bir duruma geçmek için temsilci tarafindan elde edilen bir ödülün olasiligi olan ödül olasiligi\n",
    "    5. Hemen ve gelecekteki ödülllerin önemini kontrol eden bir indirim faktörü "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2e177",
   "metadata": {},
   "source": [
    "## Donmuş Göl Uygulaması"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb555792",
   "metadata": {},
   "source": [
    "* Donmuş Göl ortamı 4x4'lük bir ortamdır.\n",
    "* 4 farklı alan bulunmaktadır.\n",
    "    1. S: Başlangıç\n",
    "    2. G: Hedef\n",
    "    3. F: Güvenli Yol\n",
    "    4. H: Delik\n",
    "* 4 farklı eylem seçeneğimiz vardır:\n",
    "    1. Sol\n",
    "    2. Sağ\n",
    "    3. İleri\n",
    "    4. Geri\n",
    "* Eğer ajan hedefe ulaşırsa ödül 1, ulaşamazsa ödül 0'dır.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb61818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1044b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ortamı çağır\n",
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6aa8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2652665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q table oluştur\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982b7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametreler\n",
    "gamma = 0.95\n",
    "alpha = 0.80\n",
    "epsilon = 0.10\n",
    "\n",
    "# Ödülleri görselleştirmek için bir liste oluştur\n",
    "reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df13cf9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     17\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Q öğrenme fonksiyonu\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# öğrenmeye başla\n",
    "\n",
    "episode_number = 100000 # 100000 bölüm olsun\n",
    "for i in range(1, episode_number):\n",
    "    \n",
    "    state = env.reset() # her bölümün başında ortamı resetle\n",
    "    reward_count = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # %10 keşif, %90 sömürü oranı\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Q öğrenme fonksiyonu\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        next_value = (1-alpha)*old_value + alpha*(reward + gamma*next_max)\n",
    "        \n",
    "        # Q tablosunu güncelle\n",
    "        q_table[state, action] = next_value\n",
    "        \n",
    "        # durum güncelle\n",
    "        state = next_state\n",
    "        \n",
    "        # toplam ödülü hesapla\n",
    "        reward_count += reward\n",
    "        \n",
    "        # eğer bölüm bittiyse kır ve yeni bölüme başla\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    if i % 5000 == 0:\n",
    "        print(\"Episode: {}\".format(i))\n",
    "    if i % 1000 == 0:\n",
    "        reward_list.append(reward_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
