{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d540c86-1782-4f80-9c4d-1273bdfca1db"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sparkSession = SparkSession.builder.appName('Spark Kafka Consumer Örnek').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac85d707-823a-4d2e-b036-e0a1a678e04a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08eb28c1-3dcb-4182-9089-02f3d2f04219"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2695144408936499#setting/sparkui/0216-114431-94wo3xpa/driver-4997686835163829617\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2695144408936499#setting/sparkui/0216-114431-94wo3xpa/driver-4997686835163829617\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["rawDF = sparkSession.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"159.223.188.228:9092\").option(\"startingOffsets\",\"earliest\").option(\"subscribe\",\"person\").load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b302535-3f5d-4654-a508-112917b85514"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rawDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"173ae1b3-942d-452d-813c-f38a01dccb51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1353366328657068>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrawDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \"\"\"\n\u001B[1;32m    489\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Queries with streaming sources must be executed with writeStream.start();\nkafka","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1353366328657068>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrawDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \"\"\"\n\u001B[1;32m    489\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4340cb2a-9961-4c62-a2d7-445a5d2b8116"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = StructType() \\\n         .add(\"name\",StringType()) \\\n         .add(\"surname\",StringType()) \\\n         .add(\"age\",IntegerType()) \\\n         .add(\"city\",StringType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b98ae660-8f53-4b31-979c-1ae84cacf016"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["parsedDF = rawDF.select(from_json(col(\"value\").cast(\"string\"),schema).alias(\"kafkadata\")).select(\"kafkadata.*\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"294992d9-328f-4388-92c9-fe68ae2d99ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["parsedDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8280d3fc-9ae3-44dc-b1bf-b7acdaeebe8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1353366328657072>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mparsedDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \"\"\"\n\u001B[1;32m    489\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Queries with streaming sources must be executed with writeStream.start();\nkafka","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-1353366328657072>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mparsedDF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    488\u001B[0m         \"\"\"\n\u001B[1;32m    489\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 490\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    491\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    492\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"]}}],"execution_count":0},{"cell_type":"code","source":["resultDF = parsedDF.filter(\"age>25\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ac75008-8a27-4b40-b681-4bb855f4b5c1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["parsedDF.createOrReplaceTempView(\"person\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fadf1fac-0262-427d-a687-2eae9b595322"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM person WHERE age>25"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a1fc830-2111-4309-99eb-011cba47caa4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Yasin","Kaya",32,"Tekirdağ"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"surname","type":"\"string\"","metadata":"{}"},{"name":"age","type":"\"integer\"","metadata":"{}"},{"name":"city","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>surname</th><th>age</th><th>city</th></tr></thead><tbody><tr><td>Yasin</td><td>Kaya</td><td>32</td><td>Tekirdağ</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["resultDF.selectExpr(\"CAST(name AS STRING) AS key\", \"to_json(struct(*)) AS value\").writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"159.223.188.228:9092\").option(\"checkpointLocation\",\"/point\").option(\"topic\",\"sonuc2\").start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4758545-069c-4342-ad57-f744fdc63619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[47]: <pyspark.sql.streaming.StreamingQuery at 0x7feacef3adf0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[47]: <pyspark.sql.streaming.StreamingQuery at 0x7feacef3adf0>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28dbcffa-a10b-406a-a421-cc89f92d0cba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"spark-kafka-consumer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1353366328657063}},"nbformat":4,"nbformat_minor":0}
